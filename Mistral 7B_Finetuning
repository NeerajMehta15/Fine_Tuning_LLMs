{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:31:47.327450Z","iopub.execute_input":"2025-02-09T04:31:47.327925Z","iopub.status.idle":"2025-02-09T04:31:47.335623Z","shell.execute_reply.started":"2025-02-09T04:31:47.327895Z","shell.execute_reply":"2025-02-09T04:31:47.334826Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/generation_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#For accessing the model\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:38:22.589083Z","iopub.execute_input":"2025-02-09T04:38:22.589382Z","iopub.status.idle":"2025-02-09T04:38:33.653929Z","shell.execute_reply.started":"2025-02-09T04:38:22.589361Z","shell.execute_reply":"2025-02-09T04:38:33.652908Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#For fine tuning the model\n!capture\n!pip install -U peft\n!pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:40:56.066875Z","iopub.execute_input":"2025-02-09T04:40:56.067223Z","iopub.status.idle":"2025-02-09T04:41:03.748764Z","shell.execute_reply.started":"2025-02-09T04:40:56.067197Z","shell.execute_reply":"2025-02-09T04:41:03.747590Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: capture: command not found\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.48.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.3.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.28.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.3.0)\nRequirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.2.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.48.3)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.28.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.11)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:32:26.747706Z","iopub.execute_input":"2025-02-09T04:32:26.748050Z","iopub.status.idle":"2025-02-09T04:32:46.837371Z","shell.execute_reply.started":"2025-02-09T04:32:26.748020Z","shell.execute_reply":"2025-02-09T04:32:46.836697Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Modules for fine tuning the model\nfrom transformers import HfArgumentParser,TrainingArguments,logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:41:04.700140Z","iopub.execute_input":"2025-02-09T04:41:04.700431Z","iopub.status.idle":"2025-02-09T04:41:05.833237Z","shell.execute_reply.started":"2025-02-09T04:41:04.700409Z","shell.execute_reply":"2025-02-09T04:41:05.832236Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#Quantization\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:33:02.346127Z","iopub.execute_input":"2025-02-09T04:33:02.346438Z","iopub.status.idle":"2025-02-09T04:33:02.351828Z","shell.execute_reply.started":"2025-02-09T04:33:02.346414Z","shell.execute_reply":"2025-02-09T04:33:02.350849Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#Accessing mistral model\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:33:05.324006Z","iopub.execute_input":"2025-02-09T04:33:05.324288Z","iopub.status.idle":"2025-02-09T04:33:05.327897Z","shell.execute_reply.started":"2025-02-09T04:33:05.324267Z","shell.execute_reply":"2025-02-09T04:33:05.327022Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:33:07.079184Z","iopub.execute_input":"2025-02-09T04:33:07.079539Z","iopub.status.idle":"2025-02-09T04:35:13.843670Z","shell.execute_reply.started":"2025-02-09T04:33:07.079496Z","shell.execute_reply":"2025-02-09T04:35:13.842995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a3cca933ca94bdba2522b3e38a121c7"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#Pipeline function to access the model\npipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer = tokenizer, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:35:28.152286Z","iopub.execute_input":"2025-02-09T04:35:28.152609Z","iopub.status.idle":"2025-02-09T04:35:28.158927Z","shell.execute_reply.started":"2025-02-09T04:35:28.152583Z","shell.execute_reply":"2025-02-09T04:35:28.158145Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"prompt = \"You are a food expert, help me with a Healthy Pizaa receipe?\"\n\nsequences = pipe(\n    prompt,\n    do_sample=True,\n    max_new_tokens=200, \n    temperature=0.7, \n    top_k=50, \n    top_p=0.95,\n    num_return_sequences=1,\n)\nprint(sequences[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:36:50.432828Z","iopub.execute_input":"2025-02-09T04:36:50.433180Z","iopub.status.idle":"2025-02-09T04:37:07.561214Z","shell.execute_reply.started":"2025-02-09T04:36:50.433156Z","shell.execute_reply":"2025-02-09T04:37:07.560352Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are a food expert, help me with a Healthy Pizaa receipe?\n\n## Healthy Pizza\n\nHi,\n\nI was wondering if you could help me with a healthy pizza receipe?\n\nI am not a food expert so I don't really know where to start.\n\nI know I am suppose to use whole wheat dough, but what else?\n\nShould I use low fat cheese?\n\nI would like to make it taste good and still be healthy.\n\nThanks,\n\nJoyce\n\n## Healthy Pizza\n\nHi Joyce,\n\nWe have a great healthy pizza recipe that is perfect for you.\n\nHere is what you need to do:\n\n1. In a large bowl, combine 1/2 cup warm water (110°F/45°C), 2-1/2 tsp. active dry yeast, and 1 tsp. honey. Let stand for 5 minutes.\n\n2. Meanwhile, combine\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_Key\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:54:06.833767Z","iopub.execute_input":"2025-02-09T04:54:06.834096Z","iopub.status.idle":"2025-02-09T04:54:07.030014Z","shell.execute_reply.started":"2025-02-09T04:54:06.834073Z","shell.execute_reply":"2025-02-09T04:54:07.028989Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#Hugging Face API to save and push the model to the Hugging Face Hub\n!huggingface-cli login --token $secret_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:54:53.620339Z","iopub.execute_input":"2025-02-09T04:54:53.620702Z","iopub.status.idle":"2025-02-09T04:54:54.232407Z","shell.execute_reply.started":"2025-02-09T04:54:53.620670Z","shell.execute_reply":"2025-02-09T04:54:54.231292Z"}},"outputs":[{"name":"stdout","text":"usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]\nhuggingface-cli <command> [<args>] login: error: argument --token: expected one argument\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"wandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:54:57.039150Z","iopub.execute_input":"2025-02-09T04:54:57.039501Z","iopub.status.idle":"2025-02-09T04:55:09.725184Z","shell.execute_reply.started":"2025-02-09T04:54:57.039475Z","shell.execute_reply":"2025-02-09T04:55:09.724355Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneerajmehta15\u001b[0m (\u001b[33mneerajmehta15-stanza-living\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250209_045503-vo0j7wp3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B/runs/vo0j7wp3' target=\"_blank\">frosty-spaceship-1</a></strong> to <a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B/runs/vo0j7wp3' target=\"_blank\">https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B/runs/vo0j7wp3</a>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"mistral_7b_guanaco\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T04:58:28.265615Z","iopub.execute_input":"2025-02-09T04:58:28.266038Z","iopub.status.idle":"2025-02-09T04:58:28.271017Z","shell.execute_reply.started":"2025-02-09T04:58:28.266008Z","shell.execute_reply":"2025-02-09T04:58:28.270206Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset[\"text\"][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:01:21.180214Z","iopub.execute_input":"2025-02-09T05:01:21.180524Z","iopub.status.idle":"2025-02-09T05:01:22.133777Z","shell.execute_reply.started":"2025-02-09T05:01:21.180501Z","shell.execute_reply":"2025-02-09T05:01:22.132807Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] cuanto es 2x2 xD [/INST] La respuesta es 4. </s><s>[INST] puedes demostrarme matematicamente que 2x2 es 4? [/INST] En una multiplicación, el producto es el resultado de sumar un factor tantas veces como indique el otro, es decir, si tenemos una operación v · n = x, entonces x será igual a v sumado n veces o n sumado v veces, por ejemplo, para la multiplicación 3 · 4 podemos sumar \"3 + 3 + 3 + 3\" o \"4 + 4 + 4\" y en ambos casos nos daría como resultado 12, para el caso de 2 · 2 al ser iguales los dos factores el producto sería \"2 + 2\" que es igual a 4 </s>'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:01:48.772028Z","iopub.execute_input":"2025-02-09T05:01:48.772330Z","iopub.status.idle":"2025-02-09T05:01:48.779107Z","shell.execute_reply.started":"2025-02-09T05:01:48.772307Z","shell.execute_reply":"2025-02-09T05:01:48.778017Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:02:00.959283Z","iopub.execute_input":"2025-02-09T05:02:00.959798Z","iopub.status.idle":"2025-02-09T05:02:08.976327Z","shell.execute_reply.started":"2025-02-09T05:02:00.959737Z","shell.execute_reply":"2025-02-09T05:02:08.975643Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4144825b43764100b0db34c6703f3212"}},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:02:38.395104Z","iopub.execute_input":"2025-02-09T05:02:38.395486Z","iopub.status.idle":"2025-02-09T05:02:39.767830Z","shell.execute_reply.started":"2025-02-09T05:02:38.395456Z","shell.execute_reply":"2025-02-09T05:02:39.767128Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:02:58.432701Z","iopub.execute_input":"2025-02-09T05:02:58.433064Z","iopub.status.idle":"2025-02-09T05:02:58.468444Z","shell.execute_reply.started":"2025-02-09T05:02:58.433038Z","shell.execute_reply":"2025-02-09T05:02:58.467455Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Ensure tokenizer padding side is set to right\ntokenizer.padding_side = \"right\"\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:10:05.918779Z","iopub.execute_input":"2025-02-09T05:10:05.919080Z","iopub.status.idle":"2025-02-09T05:10:05.994978Z","shell.execute_reply.started":"2025-02-09T05:10:05.919060Z","shell.execute_reply":"2025-02-09T05:10:05.994021Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token \n\ntrainer = SFTTrainer(\n    model=model, #Change model name\n    train_dataset=dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:11:27.892512Z","iopub.execute_input":"2025-02-09T05:11:27.892926Z","iopub.status.idle":"2025-02-09T05:11:28.478544Z","shell.execute_reply.started":"2025-02-09T05:11:27.892896Z","shell.execute_reply":"2025-02-09T05:11:28.477870Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ed287e4af84b61aa35f515a48a2194"}},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:11:30.285392Z","iopub.execute_input":"2025-02-09T05:11:30.285681Z","iopub.status.idle":"2025-02-09T06:59:25.116565Z","shell.execute_reply.started":"2025-02-09T05:11:30.285658Z","shell.execute_reply":"2025-02-09T06:59:25.115627Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 1:46:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.033100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.475100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.072900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.344000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.049200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.243200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.039200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.281000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.000800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.326200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=1.186475112915039, metrics={'train_runtime': 6473.9244, 'train_samples_per_second': 0.154, 'train_steps_per_second': 0.039, 'total_flos': 1.8706482621874176e+16, 'train_loss': 1.186475112915039, 'epoch': 1.0})"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T06:59:50.539539Z","iopub.execute_input":"2025-02-09T06:59:50.539904Z","iopub.status.idle":"2025-02-09T06:59:52.820576Z","shell.execute_reply.started":"2025-02-09T06:59:50.539877Z","shell.execute_reply":"2025-02-09T06:59:52.819854Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>▂▇▁▆▁▅▁█▁▄</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▁█▂▆▂▅▂▅▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.8706482621874176e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/grad_norm</td><td>0.82715</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>1.3262</td></tr><tr><td>train_loss</td><td>1.18648</td></tr><tr><td>train_runtime</td><td>6473.9244</td></tr><tr><td>train_samples_per_second</td><td>0.154</td></tr><tr><td>train_steps_per_second</td><td>0.039</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">frosty-spaceship-1</strong> at: <a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B/runs/vo0j7wp3' target=\"_blank\">https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B/runs/vo0j7wp3</a><br> View project at: <a href='https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/neerajmehta15-stanza-living/Fine%20tuning%20mistral%207B</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250209_045503-vo0j7wp3/logs</code>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"What is LLM fine tuning\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T07:03:11.154059Z","iopub.execute_input":"2025-02-09T07:03:11.154514Z","iopub.status.idle":"2025-02-09T07:06:24.587915Z","shell.execute_reply.started":"2025-02-09T07:03:11.154477Z","shell.execute_reply":"2025-02-09T07:06:24.587024Z"}},"outputs":[{"name":"stdout","text":"<s>[INST] What is LLM fine tuning [/INST] LLM fine tuning is a process of adapting a large language model (LLM) to a specific task or domain. This is done by training the model on a smaller dataset that is relevant to the task or domain. Fine tuning can improve the performance of the LLM on the specific task or domain, while preserving its general language understanding capabilities. 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖 🤗 🤖\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"prompt = \"How can I lean LLM fine tuning ?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T07:12:23.393974Z","iopub.execute_input":"2025-02-09T07:12:23.394314Z","iopub.status.idle":"2025-02-09T07:15:37.341918Z","shell.execute_reply.started":"2025-02-09T07:12:23.394276Z","shell.execute_reply":"2025-02-09T07:15:37.341035Z"}},"outputs":[{"name":"stdout","text":"<s>[INST] How can I lean LLM fine tuning ? [/INST] There are a few ways to fine tune a language model. One way is to use a pre-trained model as a starting point and then fine tune it on a smaller dataset. Another way is to use a pre-trained model as a starting point and then fine tune it on a larger dataset. Finally, you can also use a pre-trained model as a starting point and then fine tune it on a dataset that is similar to the task you are trying to solve. \n\nWhich of these methods you choose will depend on the specific task you are trying to solve and the resources you have available. \n\nIf you have a small dataset, you may want to use a pre-trained model as a starting point and then fine tune it on a smaller dataset. If you have a large dataset, you may want to use a pre-trained model as a starting point and then fine tune\n","output_type":"stream"}],"execution_count":65}]}